# -*- coding: utf-8 -*-
"""Knowledge-base_bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mBlgyJb3LJKOgPF404JebnYEM4ILhVhq
"""

pip install faiss-cpu

"""# Importing packages and accessing api key"""

from huggingface_hub import InferenceClient
from google.colab import userdata
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer


hf_token = userdata.get("Hugging_Face")

"""# Creating Inference client object"""

client = InferenceClient(
    token = hf_token,
    provider = "nebius"
)

"""# Initialize Embedding model"""

embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

"""# Loading documents"""

def load_documents(file_paths):
  docs = []
  for path in file_paths:
    with open(path, 'r', encoding='utf-8') as f:
      docs.append(f.read())
  return docs

"""# Creating chunks for documents"""

def chunk_text(text, chunk_size=300, overlap=50):
  words = text.split()
  chunks = []
  for i in range(0, len(words), chunk_size-overlap):
    chunk = " ".join(words[i:i+chunk_size])
    chunks.append(chunk)
  return chunks

"""# Build Faiss index

"""

def build_faiss_index(file_paths):
    docs = load_documents(file_paths)

    # Split docs into chunks
    chunks = []
    for doc in docs:
        chunks.extend(chunk_text(doc))

    # Generate embeddings
    embeddings = embedder.encode(chunks, convert_to_numpy=True)

    # Create FAISS index
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, chunks

"""# Retrieving relevant context from text"""

def retrieve_context(query, index, chunks, top_k=3):
    query_embedding = embedder.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, top_k)
    retrieved_chunks = [chunks[i] for i in indices[0]]
    return "\n\n".join(retrieved_chunks)

"""# Ask Question"""

def ask_question(query, index, chunks):
    context = retrieve_context(query, index, chunks)

    prompt = f"""
You are a Knowledge-base Q&A Bot.
Answer the question using ONLY the following context:

Context:
\"\"\"
{context}
\"\"\"

User Question:
{query}

Rules:
- If the answer exists in the context, extract it and reply clearly.
- If the answer is not found in the context, reply ONLY with: "Not found."
"""
    try:
        response = client.chat_completion(
            model="meta-llama/Llama-3.1-8B-Instruct",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error: {e}")
        return None

"""# Example Usage

"""

if __name__ == "__main__":
    files = ["company_policy.txt", "leave_policy.txt"]
    index, chunks = build_faiss_index(files)

    user_q = "What is the company's workplace conduct?"
    answer = ask_question(user_q, index, chunks)

    print("Q:", user_q)
    print("A:", answer)